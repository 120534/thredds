:source-highlighter: coderay
[[threddsDocs]]


= Encoding CDM Data in protobuf

== design goals

. *_Round trip in Java_*: ArrayStructure -> proto -> ArrayStructure
. *_Other clients_*: ArrayStructure -> proto -> python client
. *_CdmrFeature_*: Use same messages or at least same ideas in CdmrFeature protocol
. *_Ncstream file format_* (NcStreamIosp.java) : capture ncstream messages into file as valid CDM file
. *efficient*: minimize data copying, replicating metadata
. *simple*: let protobuf handle complexity
. *self-contained*: should be able to parse / recreate data just be itself

== Current design/implementation

=== ncstream grammer for data messages
----
dataMessage := MAGIC_DATA, vlenb, NcStreamProto.Data, regData | vlenData | seqData | structData

regData := vlenb, (byte)*vlenb
vlenData := vlenn, {vlenb, (byte)*vlenb}*vlenn
seqData := {MAGIC_VDATA, vlenb, NcStreamProto.StructureData}*, MAGIC_VEND
structData := vlenb, NcStreamProto.StructureData

vlenb := variable length encoded positive integer == length of the following object in bytes
vlenn := variable length encoded positive integer == number of objects that follow
----

=== Regular (not Structure) data

----
message Data {
  string varName = 1;
  DataType dataType = 2;
  Section section = 3;
  oneof bigend_present {
    bool bigend = 4;           // [default=true] in proto2
  }
  uint32 version = 5;          // version=2 for proto2, =3 for proto3 (v5.0+)
  Compress compress = 6;
  bool vdata = 7;
  uint32 uncompressedSize = 8;
}

message Range {
  uint64 start = 1;
  uint64 size = 2;
  uint64 stride = 3;
}

message Section {
  repeated Range range = 1;
}
----

* Data is not encoded in the proto message, but in data messages
* Not handling generalized Ranges
* Section specifies section in the variable shape, which is not specified in the data message, need header
* bigend_present needed for backwards compat with proto2

=== Current design for StructureData

ncStream.proto:

----
message StructureData {
  repeated uint32 member = 1; // list of members present
  bytes data = 2; // fixed length data
  repeated uint32 heapCount = 3; // heap String count
  repeated string sdata = 4; // Strings
  uint64 nrows = 5; // number of structData in this message
  uint32 rowLength = 6;  // length in bytes of each row
}
----

NcStream.java:

[source,java]
----
public static long encodeArrayStructure(ArrayStructure as, ByteOrder bo, OutputStream os) {
    // force canonical packing
    ArrayStructureBB dataBB = StructureDataDeep.copyToArrayBB(as, bo, true);

    // extract any String member data
    List<String> ss = new ArrayList<>(); # <3>
    List<Object> heap = dataBB.getHeap();
    List<Integer> count = new ArrayList<>(); # <2>
    if (heap != null) {
      for (Object ho : heap) {
        if (ho instanceof String) {
          count.add(1);
          ss.add((String) ho);
        } else if (ho instanceof String[]) {
          String[] hos = (String[]) ho;
          count.add(hos.length);
          Collections.addAll(ss, hos);
        }
      }
    }

    // LOOK optionally compress
    StructureMembers sm = dataBB.getStructureMembers();
    ByteBuffer bb = dataBB.getByteBuffer();
    NcStreamProto.StructureData proto = NcStream.encodeStructureDataProto(
        bb.array(), # <1>
        count, # <2>
        ss, # <3>
        (int) as.getSize(), # <4>
        sm.getStructureSize()); # <5>
    ...
}
----

[source,java]
----
  static NcStreamProto.StructureData encodeStructureDataProto(byte[] fixed, List<Integer> count, List<String> ss, int nrows, int rowLength) {
    NcStreamProto.StructureData.Builder builder = NcStreamProto.StructureData.newBuilder();
    builder.setData(ByteString.copyFrom(fixed)); # <1>
    builder.setNrows(nrows); # <4>
    builder.setRowLength(rowLength); # <5>
    for (Integer c : count)
      builder.addHeapCount(c); #<2>
    for (String s : ss)
      builder.addSdata(s); # <3>
    return builder.build();
  }
----
<1> fixed-length member data is stored in ByteBuffer array by row.
<2> May be a String (count = 1) or String[n] (count = n)
<3> All the Strings
<4> number or rows in ArrayStructure
<5> length of (fixed-length member data) row

I think of this as row oriented.

Issues:

* doesnt handle nested structures, opaques or vlens (yet)
* exposes data packing details, may be too complex esp when you add nested structures and vlens
** just have a byte array which client must know how to parse

== Alternative: move everything into protobuf

. We left data out of proto so that it could be streamed. Proto messages have to be constructed all at once and then serialized.
. Putting data outside of protobuf allows us to apply compression to the data

=== Allow proto messages to be compressed

Instead of

----
dataMessage := MAGIC_DATA, vlenb, NcStreamProto.Data, regData | vlenData | seqData | structData
----

use

----
dataMessage := MAGIC_DATA, compress, vlenb, NcStreamProto.Data
compress:= if NcStreamProto.Data is compressed, and with what algorithm.
----

. always read in vlenb bytes
. decompress if needed
. now apply protobuf parsing

* Alternatively, just turn on compression at the Tomcat server (remote access only not file format).
* opendap uses this; client can decompress input stream on the fly. measure speedup?

=== Make it possible to break data response into multiple messages

----
dataResponse := {dataMessage}*, MAGIC_END
----

. A data response has one or more *dataMessage* messages, with a MAGIC_END terminator

=== Move data encoding inside of NcStreamProto.Data

change

----
message Data {
  string varName = 1;
  DataType dataType = 2;
  Section section = 3;
  oneof bigend_present {
    bool bigend = 4;           // [default=true] in proto2
  }
  uint32 version = 5;          // version=2 for proto2, =3 for proto3 (v5.0+)
  Compress compress = 6;
  bool vdata = 7;
  uint32 uncompressedSize = 8;
}
----

into

----
message Data2 {
  string fullName = 1;
  DataType dataType = 2;
  Section section = 3;
  bool bigend = 4;
  uint32 version = 5;
  bool isVlen = 7;
  uint32 nelems = 9;

  // oneof
  bytes primarray = 10;        // rectangular, primitive array # <1>
  repeated string stringdata = 11;  // string dataType # <2>
  ArrayStructureDataCol structdata = 12;  // structure/seq dataType # <3>
  repeated uint32 vlens = 13;  // isVlen true # <4>
  repeated bytes opaquedata = 14;  // opaque dataType # <5>
}
----

<1> *primarray* has _nelems_ * sizeof(dataType) bytes, turn into multidim array of primitives with section info and bigend
<2> *stringdata* has _nelems_ strings, turn into multidim array of String with section info
<3> *structdata* has _nelems_ StructureData objects, turn into multidim array of StructureData with section info and bigend
<4> *vlens* has _section.size_ array lengths; section does not include the last (vlen) dimension; data in primarray
<5> *opaquedata* has _nelems_ opaque objects, turn into multidim array of Opaque with section info

Issues

. Cant use _oneof_ because it doesnt allow repeated
. backwards compatible - not sure if its possible
. using *bytes primarray* forces us to copy data twice, 1 from the primitive array to a ByteBuffer, then to a ByteString
.. lame that ByteString doesnt have a byte[] constructor, but they need immutability
.. could use different field for each primitive type (there are 7 plus 3 unsigned) for only one copy
.. we already have string and opaque, add 10 more ?
.. this would obviate bigend i think

=== Vlen

. see <<../CDM/VariableLengthData#,vlen data>>
. when encoding, the length of each vlen is known, so:
.. _float vlen(*)_ will have a known length, so can be encoded the same as a regular array
.. _float vlen(21, *)_ has 21 variable length arrays, put those lengths into vlens array
... *section* describes just the outer dimensions, section.size is length of vlens array
... *nelems* = Sum(vlens)
... *primarray* has nelems * sizeof(dataType) bytes, turn into dim array of primitives, use vlens to divide into variable length arrays

==== Vlen Language

We already have Fortran 90 syntax, and * indicating a variable length dimension. Do we really want to support arbitrary vlen dimension ??

* array(outer, *)
* array(*, inner)
* array(outer, *, inner)

An obvious thing to do is to use java/C "array of arrays". rather than Fortran / netCDF rectangular arrays:

* array[outer][*]
* array[*][inner]
* array[outer][*][inner]

what does numPy do ??

java/C assumes in memory. Is this useful for very large, ie out of memory, data?

Nested Tables has taken approach that its better to use Structures rather than arrays, since there are usually multiple fields. Fortran programmers
prefer arrays, but they are thinking of in memory.

What is the notation that allows a high level specification (eg SQL), that can be efficiently executed by a machine ?

Extending the array model to very large datasets may not be appropriate. Row vs column store.

What about a transform language on the netcdf4 / CDM data model, to allow efficient rewriting of data ? Then it also becomes an extraction language ??


=== StructureData

Possible protobuf encoding:

----
message ArrayStructureDataCol {
  repeated Data2 memberData = 1;
  repeated uint32 shape = 3; // needed?
}
----

. this is column oriented data storage (see below)

=== Column oriented storage for Nested Structures

----
Structure {
  int fld1
  string fld2(12);
  Structure {
    float fld3;
    long fld4(2,3);
  } inner(99)
} s(123)
----

can be encoded like:

----
  int s.fld1(123)
  string s.fld2(123, 12);
  float s.inner.fld3(123,99);
  long s.inner.fld4(123,99,2,3);
----

* The shape of member data includes the outer structure(s).
* All of the data resides at the innermost Structure.
* Or one could flatten the structure members to eliminate nesting, rely on parsing the name to reconstruct nested structures

=== Vlen Structures

----
Structure {
  int fld1
  string fld2(12);
  Structure {
    float fld3;
    long fld4(2,3);
  } inner(99)
} s(*)
----

makes a vlen in the outer dimension:

----
  int s.fld1(*)
  string s.fld2(*, 12);
  float s.inner.fld3(*,99);
  long s.inner.fld4(*,99,2,3);
----

* not actually a problem because at encoding time we know what * is.
* still, some inefficiency in processing, have to read all into memory first
* would be better to go back to row oriented, like:
** VLEN_START, row, row, row... VLEN_END
** repeated bytes

Also

----
Structure {
  int fld1
  string fld2(12);
  Structure {
    float fld3;
    long fld4(2,3);
  } inner(*)
} s(123)
----

might make a vlen in a middle dimension:

----
  int s.fld1(123)
  string s.fld2(123, 12);
  float s.inner.fld3(123,*);
  long s.inner.fld4(123,*,2,3);
----

* Could require that Structures can only use vlen if 1D, eliminating Structure s(123,*)
* Then a vlen Structure must be a Sequence
* Then we code Sequences in a special way?

* Making up a seperate Data message for each StructureData is too much overhead?
* Could create a general encoding for vlens of arbitrary placement
** Vlen is a List, non-vlen is an array
* Would like protobuf to reflect this


=== Vlens inside of structures

These can only make other vlens (last dimension a vlen)

----
Structure {
  int fld1
  string fld2(*);
  Structure {
    float fld3;
    long fld4(2,*);
  } inner(99)
} s(33)
----

makes:

----
  int s.fld1(33)
  string s.fld2(33, *);
  float s.inner.fld3(33,99);
  long s.inner.fld4(33,99,2,*);
----

so i think not a problem; just have to deal with sequences

=== Sequence

Sequence has these considerations:

. section is not used
. nelems is used to specify how many StructureData objects are in the Data message. This is the value of (*)
. There may be multiple Data messages
.. Anticipate buffering, say, 1000 StructureData objects into a Data message, and send a sequence of messages until done
. Nested Sequences must be complete (?)
.. Allow user to request nested Sequence, which then could be sent in multiple messages
.. Allow user to select what members are sent, to skip nested sequences if they are too large

Would be best to have a row-orientaton. Each row could be  a byte array. The list of Members would be sufficient to parse the byte array.

=== Issues

. Backwards compatibility with versions < 5 ?
. Difficulty of parsing in python client
. should data be in protobuf? not sure of overhead
.. ok as streaming protocol, less so as file format

== data transfer dods vs cdm, with/without compression

----
filename scanCdmUnitTests/formats/grib1/Mercator.grib1
  CDM : avg=11.4 std=1.46 (n=10) MB/sec
  DODS: avg=12.6 std=.337 (n=10) MB/sec
  CDM2: avg=13.5 std=1.72 (n=10) MB/sec
  CDM3: avg=12.3 std=1.75 (n=10) MB/sec
  CDM4: avg=13.4 std=1.75 (n=10) MB/sec

compress:
  CDM : avg=8.75 std=1.06 (n=10) MB/sec
  DODS: avg=2.07 std=.0464 (n=10) MB/sec
  compression ratio = 2.592550
----
----
filename scanCdmUnitTests/formats/grib2/ds.pop12.bin
  CDM : avg=14.7 std=1.12 (n=10) MB/sec
  DODS: avg=17.6 std=1.87 (n=10) MB/sec
  CDM2: avg=15.4 std=.497 (n=10) MB/sec
  CDM3: avg=13.0 std=1.66 (n=10) MB/sec
  CDM4: avg=15.4 std=.690 (n=10) MB/sec

compress:
  CDM : avg=38.1 std=.654 (n=10) MB/sec
  DODS: avg=24.5 std=.988 (n=10) MB/sec
  compression ratio = 49.604264
----
----
filename scanCdmUnitTests/formats/grib2/AVOR_000.grb
  CDM : avg=8.96 std=.934 (n=10) MB/sec
  DODS: avg=10.1 std=.470 (n=10) MB/sec
  CDM2: avg=9.45 std=.519 (n=10) MB/sec
  CDM3: avg=8.77 std=.548 (n=10) MB/sec
  CDM4: avg=9.38 std=.210 (n=10) MB/sec

compress:
  CDM : avg=8.94 std=.374 (n=10) MB/sec
  DODS: avg=2.92 std=.0769 (n=10) MB/sec
  compression ratio = 3.506321

----
----
filename scanCdmUnitTests/conventions/cf/bora_feb_001.nc
  CDM : avg=10.7 std=.817 (n=10) MB/sec
  DODS: avg=10.3 std=1.51 (n=10) MB/sec
  CDM2: avg=11.9 std=.637 (n=10) MB/sec
  CDM3: avg=11.5 std=.865 (n=10) MB/sec
  CDM4: avg=11.5 std=1.08 (n=10) MB/sec

compress:
 CDM : avg=8.14 std=.361 (n=10) MB/sec
 DODS: avg=1.81 std=.0522 (n=10) MB/sec
 compression ratio = 1.533374

----
----
filename scanCdmUnitTests/conventions/cf/ccsm2.nc
  CDM : avg=11.8 std=.412 (n=10) MB/sec
  DODS: avg=11.2 std=.461 (n=10) MB/sec
  CDM2: avg=13.6 std=.383 (n=10) MB/sec
  CDM3: avg=12.7 std=.302 (n=10) MB/sec
  CDM4: avg=13.2 std=.228 (n=10) MB/sec

compress:
  CDM : avg=9.34 std=.435 (n=10) MB/sec
  DODS: avg=1.94 std=.0773 (n=10) MB/sec
  compression ratio = 1.600860
----
----
total

  CDM : avg=11.5 std=2.14 (n=50) MB/sec
  DODS: avg=12.4 std=2.97 (n=50) MB/sec
  CDM2: avg=12.8 std=2.19 (n=50) MB/sec
  CDM3: avg=11.7 std=1.95 (n=50) MB/sec
  CDM4: avg=12.6 std=2.25 (n=50) MB/sec

compress:
  CDM : avg=14.6 std=11.7 (n=50) MB/sec
  DODS: avg=6.65 std=8.95 (n=50) MB/sec
----

=== Conclusions

==== compression

. ds.pop12.bin distorts compression result, in fact most dods files are very bad (tomcat?), and cdm somewhat worse in compression
. client/server on the same machine, so not bandwidth limited; need to test compression across network
. needs more study on usefullness, for now client leave default off, server compress if asked.

==== CDM algo

. not clear if these results are statistically significant, but they seem to be stable.
. CDM2 10% faster in this test.
..    CDM: current, with data outside of proto. not sure why proto is faster. ByteBuffer.wrap() does not have to make extra copy.
..    CDM2: proto with byte array, has extra copy ByteString -> ByteBuffer -> Array
..    CDM3: proto with repeating arrays, converts to Objects, proto -> ArrayList<Object> -> Array.
..    CDM4: same as CDM2, using writeToDelimited() and parseFromDelimited() - handles message delimiting

==== CDM vs DODS

. not clear if these results are statistically significant, but they seem to be stable.
. ignore compress for now.
